---
title : Assignment 0
author : Student Name and \#
options:
  eval: true #Set this to true if you'd like to evaluate the code in this document
---

The primary goal of this assignment is to allow you to practice and assess your prerequisite knowledge which will be relied on throughout this course.
The secondary objective is to familiarize you with the tools and best practices, including:
* Mathematical typesetting (LaTeX)
* Version control (git and github)
* Unit testing
* Setting random seeds for reproducibility
* Automatic differentiation

The starter code and examples below are in the Julia programming language. You may also submit solutions using Python, if that is more familiar for you.

You are expected to submit a typeset (LaTeX) **write-up** (pdf) that contains everything that will be assessed. In particular, this means your writeup must include
* Important source code. If a question asks you to implement a piece of code, include it in the writeup. Make this clear for the marker, don't just append your entire source code into the pdf.
* Outputs from the code. If a question asks you to report some values, those must be included in the writeup.
* Plots must be included in the writeup, and be clearly labelled (title, axes, legend, caption).
* Unit tests. For some questions where you implement a piece of code, you will be expected to test the correctness of that code. Include your unit tests in your writeup.

You will also be expected to include **all source code** along with your writeup.
However, graders will not be expected to run your source code.

Questions where you are asked to run unit tests may require you to produce the unit test. For example, in question 2.1 you will manually write the derivates for various functions.
In question 2.2 you will use Automatic Differentiation to compute derivatives of those same functions.
You will test the correctness of these answers by producing unit tests for each question.
This is a very useful practice because it's possible that either your code or your math may be incorrect, but it's much less likely (still possible) that both are incorrect for the same reasons!

If you are using the Julia starter code I have included all the packages you will need in the repo.
You can activate those packages in the command-line by starting the julia session with `julia --project`
or if you are already in the REPL (like in Atom) by opening the package manager (by typing `[` into the REPL) and activating the project `[ activate .` (the period is part of the command).
If you've done this correctly, when you open the Package manager (type `]`) you should see `(assignment_0) pkg>`.

This document is as example of [literate programming](https://en.wikipedia.org/wiki/Literate_programming), which [weave](http://weavejl.mpastell.com/stable/)s together text (markdown), math (LaTeX), and code (julia) from a single document.
The source for this write-up can be found in `A0.jmd` and can be produced using `make_pdf.jl`.
You may use this to produce your own writeups, but this is not required.
Feel free to use LaTeX as normal, and include the relevant source code, outputs, and plots.

```julia
# We will use unit testing to make sure our solutions are what we expect
# This shows how to import the Test package, which provides convenient functions like @test
using Test
# Setting a Random Seed is good practice so our code is consistent between runs
using Random # Import Random Package
Random.seed!(414); #Set Random Seed
# ; suppresses output, makes the writeup slightly cleaner.
# ! is a julia convention to indicate the function mutates a global state.
```

# Probability

## Variance and Covariance
Let $X$ and $Y$ be two continuous, independent random variables.

1. [3pts] Starting from the definition of independence, show that the independence of $X$ and $Y$ implies that their covariance is $0$.

Answer:

If $X$ and $Y$ are independent variables, then $\mathbb{E}(XY) = \mathbb{E}(X)\mathbb{E}(Y)$. From the definition of covariance:

$$
\begin{align*}
    Cov(X, Y) &= \mathbb{E}\big[(X - \mathbb{E}[X])(Y - \mathbb{E}[Y])\big] \\
    &= \mathbb{E}\big[XY - X\mathbb{E}[Y] - \mathbb{E}[X]Y + \mathbb{E}[X]\mathbb{E}[Y]\big] \\
    &= \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y] - \mathbb{E}[X]\mathbb{E}[Y] + \mathbb{E}[X]\mathbb{E}[Y] \\
    &= \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y]\\
    &= \mathbb{E}[X]\mathbb{E}[Y] - \mathbb{E}[X]\mathbb{E}[Y]\\
    &= 0
\end{align*}
$$

Thus we have shown that if $X$ and $Y$ are independent variables, $Cov(X,Y)=0$.

2. [3pts] For a scalar constant $a$, show the following two properties starting from the definition of expectation:

$$
\begin{align}
\mathbb{E}(X+aY) &= \mathbb{E}(X) + a\mathbb{E}(Y)\\
\text{var}(X + aY) &= \text{var}(X) + a^2 \text{var}(Y)
\end{align}
$$

Answer:

$$
\begin{align*}
    &\text{Expected value for a descrete random variables:}\\
    \mathbb{E}[X + \alpha Y] &= \sum_{i} \sum_{j} (x_{i} + \alpha y_{j}) P(X = x_{i}, Y = y_{j}) \\
    &= \sum_{i} \sum_{j} x_{i} P(X = x_{i}, Y = y_{j}) + \sum_{i} \sum_{j} \alpha y_{j} P(X = x_{i}, Y = y_{j})\\
    &= \sum_{i} x_{i} P(X = x_{i}) + \sum_{j} \alpha y_{j} P(Y = y_{j})\\
    &= \sum_{i} x_{i} P(X = x_{i}) + \alpha\sum_{j} y_{j} P(Y = y_{j})\\
    &= \mathbb{E}(X) + \alpha \mathbb{E}(Y)\\\\
    &\text{Expected value for a continuous random variable:}\\
    \mathbb{E}(X + \alpha Y) &= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} (x + \alpha y) f(x, y) dx dy \\
    &= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} x f(x, y) dx dy + \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \alpha y f(x, y) dx dy \\
    &= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} x f(x, y) dx dy + \alpha \int_{-\infty}^{\infty} \int_{-\infty}^{\infty}  y f(x, y) dx dy \\
    &= \int_{-\infty}^{\infty} x f(x) dx + \alpha \int_{-\infty}^{\infty}  y f(y) dy \\
    &= \mathbb{E}[X] + \alpha \mathbb{E}[Y] \\\\
    Var[X + \alpha Y] &= \mathbb{E} \left[ \bigg((X + \alpha Y) - \mathbb{E}[X + \alpha Y] \bigg)^2 \right] \\
    &= \mathbb{E}\left[ \bigg(X + \alpha Y - \mathbb{E}[X] - \alpha\mathbb{E}[Y] \bigg)^2 \right]\\
    &= \mathbb{E}\left[\bigg((X - \mathbb{E}[X]) + \alpha(Y - \mathbb{E}[Y] )\bigg)^2 \right]\\
    &= \mathbb{E}\left[(X - \mathbb{E}[X])^2 + 2\alpha(X - \mathbb{E}[X])(Y - \mathbb{E}[Y]) + \alpha^2(Y - \mathbb{E}[Y])^2 \right]\\
    &= Var(X)^2 + 2\alpha Cov(X, Y) + \alpha^2 Var(Y)\\
    &= Var(X)^2 + \alpha^2 Var(Y)\\
\end{align*}
$$

Thus we have shown the two properties from the definition of expected value.

## 1D Gaussian Densities

1. [1pts] Can a probability density function (pdf) ever take values greater than $1$?

Answer:

A valid probability density function (pdf) $f(x)$ has the following properties:
1) $f(x) \geq 0$
2) $\int_{-\infty}^{\infty}f(x)dx = 1$
A pdf can take values greater than 1 as long as it satisfies these two properties. An example of such a distribution is a uniform distribution from $0 < x < 1/2. f(x) = 2$.

2.  Let $X$ be a univariate random variable distributed according to a Gaussian distribution with mean $\mu$ and variance $\sigma^2$.

* [[1pts]] Write the expression for the pdf:

Answer:

The pdf for a univariate gaussian distribution $X$, mean $\mu$, and variance $\sigma^2$ is given by the equation:

$$
\begin{align*}
   f(x,\mu,\sigma^2) = \dfrac{1}{\sqrt{2\pi\sigma^2}}e^{-\dfrac{(x-\mu)^2}{2\sigma^2}}
\end{align*}
$$


* [[2pts]] Write the code for the function that computes the pdf at $x$ with default values $\mu=0$ and $\sigma = \sqrt{0.01}$:

    Answer:

```julia
function gaussian_pdf(x; mean=0., variance=0.01)
  # return #TODO: implement pdf at x
  return 1/(2*Base.MathConstants.pi*variance)^0.5 * Base.MathConstants.e^(-0.5*(x-mean)^2/variance)
end
```

Test your implementation against a standard implementation from a library:
```julia
# Test answers
using Distributions: pdf, Normal # Note Normal uses N(mean, stddev) for parameters
@testset "Implementation of Gaussian pdf" begin
  x = randn()
  @test gaussian_pdf(x) ≈ pdf.(Normal(0.,sqrt(0.01)),x)
  # ≈ is syntax sugar for isapprox, typed with `\approx <TAB>`
  # or use the full function, like below
  @test isapprox(gaussian_pdf(x,mean=10., variance=1) , pdf.(Normal(10., sqrt(1)),x))
end;
```

3. [1pts] What is the value of the pdf at $x=0$? What is probability that $x=0$ (hint: is this the same as the pdf? Briefly explain your answer.)

Answer:

The pdf of a normal distribution with mean $\mu$, and variance $\sigma^2$ when $x = 0$ is:

$$
\begin{align*}
   f(0,\mu,\sigma^2)
   &= \dfrac{1}{\sqrt{2\pi\sigma^2}}e^{-\dfrac{(0-\mu)^2}{2\sigma^2}} \\
   &= \dfrac{1}{\sqrt{2\pi\sigma^2}}e^{-\dfrac{\mu^2}{2\sigma^2}} \\
\end{align*}
$$

Since the probability of a normal distribution is the area under the curve, it can be found as an integral over the interval $a$ to $b$:

$$
\begin{align*}
   P(a \leq x \leq b) = \int_{a}^{b} f(x,\mu,\sigma^2) dx \\
\end{align*}
$$

And it is readily apparent that to for $x = 0$, the interval from $a$ to $b$ is 0 (since $a = b = 0$). Since the integral over an interval of 0 is 0,
The probability of a normal distribution when $x = 0$ is also 0.

4. A Gaussian with mean $\mu$ and variance $\sigma^2$ can be written as a simple transformation of the standard Gaussian with mean $0.$ and variance $1.$.

  * [[1pts]] Write the transformation that takes $x \sim \mathcal{N}(0.,1.)$ to $z \sim \mathcal{N}(\mu, \sigma^2)$:

    Answer:

    The transformation is $x = \dfrac{z - \mu}{\sigma}$ (or $z = x\sigma + \mu$). This can be verified. Starting with $x \sim \mathcal{N}(0, 1)$:

$$
\begin{align*}
     & \int_{-\infty}^{x}\dfrac{1}{\sqrt{2\pi}}e^{-\dfrac{x^2}{2}}dx \\
     & \text{Sub in $x = \dfrac{z-\mu}{\sigma}$, $dx = \dfrac{1}{\sigma}dz$} \\
     &= \int_{-\infty}^{\dfrac{z-\mu}{\sigma}}\dfrac{1}{\sigma\sqrt{2\pi}}e^{-\dfrac{(z-\mu^2)}{2\sigma^2}}dz \\
     &= \int_{-\infty}^{\dfrac{z-\mu}{\sigma}}\dfrac{1}{\sqrt{2\pi\sigma^2}}e^{-\dfrac{(z-\mu^2)}{2\sigma^2}}dz \\
\end{align*}
$$

The last line denotes a sampling $z \sim \mathcal{N}(\mu, \sigma^2)$. Thus it is apparent that $x = \dfrac{z - \mu}{\sigma}$ transforms $x \sim \mathcal{N}(0, 1)$ to $z \sim \mathcal{N}(\mu, \sigma^2)$.

  * [[2pts]] Write a code implementation to produce $n$ independent samples from $\mathcal{N}(\mu, \sigma^2)$ by transforming $n$ samples from $\mathcal{N}(0.,1.)$.

    Answer

```julia
function sample_gaussian(n; mean=0., variance=0.01)
  #TODO: samples from standard gaussian
  x = randn(1,n)

  # TODO: transform x to sample z from N(mean,variance)
  z = vec([sqrt(variance) * x_i + mean for x_i in x])
  return z
end;
```

[2pts] Test your implementation by computing statistics on the samples:

```julia
using Statistics: mean, var
@testset "Numerically testing Gaussian Sample Statistics" begin
  #TODO: Sample 100000 samples with your function and use mean and var to
  # compute statistics.
  # tests should compare statistics against the true mean and variance from arguments.
  # hint: use isapprox with keyword argument atol=1e-2
  z = sample_gaussian(100000, mean = 0, variance = 0.01)
  @test isapprox(mean(z), 0., atol=1e-2)
  @test isapprox(var(z), 0.01, atol=1e-2)
end;
```

5. [3pts] Sample $10000$ samples from a Gaussian with mean $10.$ an variance $2$. Plot the **normalized** `histogram` of these samples. On the same axes `plot!` the pdf of this distribution.
Confirm that the histogram approximates the pdf.
(Note: with `Plots.jl` the function `plot!` will add to the existing axes.)

```julia;fig_cap="Histogram of Gaussian Samples"
using Plots
using Distributions: pdf, Normal

#histogram(#TODO)
#plot!(#TODO)
function sample_gaussian(n; mean=0., variance=0.01)
  #TODO: samples from standard gaussian
  x = randn(1,n)

  # TODO: transform x to sample z from N(mean,variance)
  z = vec([sqrt(variance) * x_i + mean for x_i in x])
  return z
end;

histogram(5:0.01:15, sample_gaussian(10000, mean = 10, variance = 2), normalize=true, label="Histogram of Gaussian Samples", ylabel="Density", xlabel="Value", title="Histogram of Gaussian Samples")
plot!(5:0.01:15, pdf.(Normal(10,2),5:0.01:15), label="PDF of Gaussian Distribution")
```

# Calculus

## Manual Differentiation

Let $x,y \in \mathbb{R}^m$, $A \in \mathbb{R}^{m \times n}$, and square matrix $B \in \mathbb{R}^{m \times m}$.
And where $x'$ is the transpose of $x$.
Answer the following questions in vector notation.

Let $x$ =
    $\begin{bmatrix}
    x_1 \\
    x_2 \\
    \vdots \\
    x_n
    \end{bmatrix}$, $y$ =
    $\begin{bmatrix}
    y_1 \\
    y_2 \\
    \vdots \\
    y_n \\
    \end{bmatrix}$,
    $A$ =
    $\begin{bmatrix}
    a_{1 1} & a_{1 2} & \cdots & a_{1 n} \\
    a_{2 1} & a_{2 2} & \cdots & a_{2 n} \\
    \vdots & \vdots & \ddots & \vdots \\
    a_{m 1} & a_{m 2} & \cdots & a_{m n} \\
    \end{bmatrix}$,
    $B$ =
    $\begin{bmatrix}
    b_{1 1} & b_{1 2} & \cdots & b_{1 m} \\
    b_{2 1} & b_{2 2} & \cdots & b_{2 m} \\
    \vdots & \vdots & \ddots & \vdots \\
    b_{m 1} & b_{m 2} & \cdots & b_{m m} \\
    \end{bmatrix}$,

1. [1pts] What is the gradient of $x'y$ with respect to $x$?

Answer:

$$
\begin{align*}
    x'y &= x_{1}y_{1} + x_{2}y_{2} + ... + x_{m}y_{m} \\
    \nabla x'y &= \left( \dfrac{\partial (x_{1}y_{1} + x_{2}y_{2} + ... + x_{m}y_{m})}{\partial x_{1}}, \dfrac{\partial (x_{1}y_{1} + x_{2}y_{2} + ... + x_{m}y_{m})}{\partial x_{2}}, ..., \dfrac{\partial (x_{1}y_{1} + x_{2}y_{2} + ... + x_{m}y_{m})}{\partial x_{m}}  \right) \\
    &= \left( \dfrac{\partial x_{1}y_{1}}{\partial x_{1}}, \dfrac{\partial x_{2}y_{2}}{\partial x_{2}}, ..., \dfrac{\partial x_{m}y_{m}}{\partial x_{m}}  \right) \\
    &= \left( y_{1}, y_{2}, ..., y_{m} \right) \\
    &= y\\
\end{align*}
$$

2. [1pts] What is the gradient of $x'x$ with respect to $x$?

Answer:

$$
\begin{align*}
    x'x &= x_{1}^{2} + x_{2}^{2} + ... + x_{m}^{2} \\
    \nabla x'x &= \left( \dfrac{\partial (x_{1}^{2} + x_{2}^{2} + ... + x_{m}^{2})}{\partial x_{1}}, \dfrac{\partial (x_{1}^{2} + x_{2}^{2} + ... + x_{m}^{2})}{\partial x_{2}}, ..., \dfrac{\partial (x_{1}^{2} + x_{2}^{2} + ... + x_{m}^{2})}{\partial x_{m}}  \right) \\
    &= \left( \dfrac{\partial x_{1}^{2}}{\partial x_{1}}, \dfrac{\partial x_{2}^{2}}{\partial x_{2}}, ..., \dfrac{\partial x_{m}^{2}}{\partial x_{m}}  \right) \\
    &= \left( 2x_{1}, 2x_{2}, ..., 2x_{m} \right) \\
    &= 2x \\
\end{align*}
$$

3. [2pts] What is the Jacobian of $x'A$ with respect to $x$?

Answer:

$$
\begin{align*}
    x'A &=
    \begin{bmatrix}
        x_{1} a_{1 1} + ... +  x_{m} a_{m 1} & x_{1} a_{1 2} + ... +  x_{m} a_{m 2} & \cdots & x_{1} a_{1 n} + ... +  x_{m} a_{m n} \\
    \end{bmatrix} \\
    J_{x'A} &=
    \begin{bmatrix}
        \dfrac{\partial (x_{1} a_{1 1} + ... +  x_{m} a_{m 1})}{\partial x_{1}} & \dfrac{\partial (x_{1} a_{1 1} + ... +  x_{m} a_{m 1})}{\partial x_{2}} & \cdots & \dfrac{\partial (x_{1} a_{1 1} + ... +  x_{m} a_{m 1})}{\partial x_{m}} \\
        \dfrac{\partial (x_{1} a_{1 2} + ... +  x_{m} a_{m 2})}{\partial x_{1}} & \dfrac{\partial (x_{1} a_{1 2} + ... +  x_{m} a_{m 2})}{\partial x_{2}} & \cdots & \dfrac{\partial (x_{1} a_{1 2} + ... +  x_{m} a_{m 2})}{\partial x_{m}} \\
        \vdots & \vdots & \ddots & \vdots \\
        \dfrac{\partial (x_{1} a_{1 n} + ... +  x_{m} a_{m n})}{\partial x_{1}} & \dfrac{\partial (x_{1} a_{1 n} + ... +  x_{m} a_{m n})}{\partial x_{2}} & \cdots & \dfrac{\partial (x_{1} a_{1 n} + ... +  x_{m} a_{m n})}{\partial x_{m}} \\
    \end{bmatrix} \\
    &=
    \begin{bmatrix}
        \dfrac{\partial x_{1} a_{1 1}}{\partial x_{1}} & \dfrac{\partial x_{2} a_{2 1}}{\partial x_{2}} & \cdots & \dfrac{\partial x_{m} a_{m 1}}{\partial x_{m}} \\
        \dfrac{\partial x_{1} a_{1 2}}{\partial x_{1}} & \dfrac{\partial x_{2} a_{2 2}}{\partial x_{2}} & \cdots & \dfrac{\partial x_{m} a_{m 2}}{\partial x_{m}} \\
        \vdots & \vdots & \ddots & \vdots \\
        \dfrac{\partial x_{1} a_{1 n}}{\partial x_{1}} & \dfrac{\partial x_{2} a_{2 n}}{\partial x_{2}} & \cdots & \dfrac{\partial x_{m} a_{m n}}{\partial x_{m}} \\
    \end{bmatrix} \\
    &=
    \begin{bmatrix}
        a_{1 1} & a_{2 1} & \cdots & a_{m 1} \\
        a_{1 2} & a_{2 2} & \cdots & a_{m 2} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{1 n} & a_{2 n} & \cdots & a_{m n} \\
    \end{bmatrix} \\
    &= A^{T}
\end{align*}
$$

4. [2pts] What is the gradient of $x'Bx$ with respect to $x$?

Answer:

$$
\begin{align*}
    x'B &=
    \begin{bmatrix}
        x_{1} b_{1 1} + ... +  x_{m} b_{m 1} & x_{1} b_{1 2} + ... +  x_{m} b_{m 2} & \cdots & x_{1} b_{1 n} + ... +  x_{m} b_{m n} \\
    \end{bmatrix} \\
    x'Bx &= (x_{1} b_{1 1} + ... +  x_{m} b_{m 1}) x_{1} + (x_{1} b_{1 2} + ... +  x_{m} b_{m 2}) x_{2} + ... + (x_{1} b_{1 n} + ... +  x_{m} b_{m n}) x_{m} \\
    &= (x_{1}^2 b_{1 1} + x_{1} x_{2} b_{2 1} + ... +  x_{1} x_{m} b_{m 1}) + (x_{1} x_{2} b_{1 2} + x_{2}^2 b_{2 2} + ... +  x_{2} x_{m} b_{m 2}) + ... + (x_{1} x_{m} b_{1 m} \\
    & + x_{2} x_{m} b_{2 m} + ... +  x_{m}^2 b_{m m}) \\
    \nabla x'Bx &=
    \begin{bmatrix}
        2x_{1}b_{1 1}+x_{2}(b_{2 1} + b_{1 2}) + ... + x_{m}(b_{m 1} + b_{1 m}) \\
        x_{1}(b_{2 1} + b_{1 2})+2x_{2}b_{2 2} + ... + x_{m}(b_{m 2} + b_{2 m}) \\
        \vdots \\
        x_{1}(b_{m 1} + b_{1 m}) + x_{2}(b_{m 2} + b_{2 m}) + ... + 2x_{m}b_{m m} \\
    \end{bmatrix} \\
    &=
    \begin{bmatrix}
        x_{1}b_{1 1} + x_{2}b_{2 1} + ... + x_{m}b_{m 1} \\
        x_{1}b_{2 1} + 2x_{2}b_{2 2} + ... + x_{m}b_{m 2} \\
        \vdots \\
        x_{1}b_{m 1} + x_{2}b_{m 2} + ... + x_{m}b_{m m} \\
    \end{bmatrix}
    +
    \begin{bmatrix}
        x_{1}b_{1 1} + x_{2}b_{1 2} + ... + x_{m}b_{1 m} \\
        x_{1}b_{1 2} + x_{2}b_{2 2} + ... + x_{m}b_{2 m} \\
        \vdots \\
        x_{1}b_{1 m} + x_{2}b_{2 m} + ... + x_{m}b_{m m} \\
    \end{bmatrix} \\
    &= (x'B)' + (x'B')' \\
    &= B'x + Bx \\
\end{align*}
$$

## Automatic Differentiation (AD)

Use one of the accepted AD library (Zygote.jl (julia), JAX (python), PyTorch (python))
to implement and test your answers above.

### [1pts] Create Toy Data


```julia;eval=true
# Choose dimensions of toy data
m = 3
n = 5

# Make random toy data with correct dimensions
x = vec(rand(m, 1))
y = vec(rand(m, 1))
A = rand(m, n)
B = rand(m, m)
```
[1pts] Test to confirm that the sizes of your data is what you expect:
```julia;eval=true
# Make sure your toy data is the size you expect!
@testset "Sizes of Toy Data" begin
  #TODO: confirm sizes for toy data x,y,A,B
  #hint: use `size` function, which returns tuple of integers.
  @test size(x) == (m,)
  @test size(y) == (m,)
  @test size(A) == (m, n)
  @test size(B) == (m, m)
end;
```

### Automatic Differentiation

1. [1pts] Compute the gradient of $f_1(x) = x'y$ with respect to $x$?

```julia
# Use AD Tool
using Zygote: gradient
# note: `Zygote.gradient` returns a tuple of gradients, one for each argument.
# if you want just the first element you will need to index into the tuple with [1]
f1(x) = (x' * y)[1]
df1dx = gradient(x-> f1(x), x)[1]
```

2. [1pts] Compute the gradient of $f_2(x) = x'x$ with respect to $x$?

```julia
f2(x) = (x' * x)[1]
df2dx = gradient(x-> f2(x), x)[1]
```

3. [1pts] Compute the Jacobian of $f_3(x) = x'A$ with respect to $x$?

If you try the usual `gradient` fucntion to compute the whole Jacobian it would give an error.
You can use the following code to compute the Jacobian instead.

```julia
function jacobian(f, x)
    y = f(x)
    n = length(y)
    m = length(x)
    T = eltype(y)
    j = Array{T, 2}(undef, n, m)
    for i in 1:n
        j[i, :] .= gradient(x -> f(x)[i], x)[1]
    end
    return j
end
```

[2pts] Briefly, explain why `gradient` of $f_3$ is not well defined (hint: what is the dimensionality of the output?) and what the `jacobian` function is doing in terms of calls to `gradient`.
Specifically, how many calls of `gradient` is required to compute a whole `jacobian` for $f : \mathbb{R}^m \rightarrow \mathbb{R}^n$?

Answer:

`gradient` of $f_3$ is not well defined because $f_3$ it returns a $1 \times m$ vector whereas `gradient` can only take in a function with a scalar output.
The `jacobian` function first creates an empty $n \times m$ matrix. Then it loops through the rows and sets each row to the output of `gradient` of $f_3$ before returning it as an output.
It takes a total of $n$ calls of `gradient` for the function to compute the jacobian.

The very important takeaway here is that, with AD, `gradient`s are cheap but full `jacobian`s are expensive.

```julia
f3(x) = x' * A
df3dx = jacobian(f3, x)
```

4. [1pts] Compute the gradient of $f_4(x) = x'Bx$ with respect to $x$?

```julia
f4(x) = x' * B * x
df4dx = gradient(x -> f4(x), x)[1]
```


5. [2pts] Test all your implementations against the manually derived derivatives in previous question
```julia
# Test to confirm that AD matches hand-derived gradients
@testset "AD matches hand-derived gradients" begin
  @test df1dx == y
  @test df2dx == 2 * x
  @test df3dx == A'
  @test df4dx == B' * x + B * x
end
```
